{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 플젠 - 창업 / 사이드 프로젝트 아이디어 생성 AI v0.1\n",
        "\n",
        "플젠은 새로운 프로젝트 아이디어가 떠오르지 않을 때 사용자의 특성과 시장성 등을 고려해 적합한 아이디어 제안해 주는 AI 입니다.\n",
        "\n",
        "v0.1 에서는 사용자가 입력한 키워드에 대해 아이디어를 생성하는 기능까지 구현했습니다."
      ],
      "metadata": {
        "id": "jI44YWVW3_Zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 데이터 수집\n",
        "\n"
      ],
      "metadata": {
        "id": "PtpfrE0s5hcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. N 플랫폼\n",
        "스타트업 소개, 투자유치, 제품 정보 제공 플랫폼에서 데이터"
      ],
      "metadata": {
        "id": "CFZTPpeZ5nka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 전체 1만8천개 스타트업 URL 수집\n"
      ],
      "metadata": {
        "id": "x7UOuiqq6GFN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-CggJdP3vJK"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!pip install chromedriver-autoinstaller\n",
        "!pip install webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "import chromedriver_autoinstaller\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from selenium.webdriver import ActionChains\n",
        "from tqdm import tqdm_notebook as tq"
      ],
      "metadata": {
        "id": "JqUaWNWY7qb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chrome_options = Options()\n",
        "chrome_options.add_experimental_option(\"detach\", True)\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
        "service = Service(executable_path=ChromeDriverManager().install())\n",
        "caps = DesiredCapabilities.CHROME\n",
        "caps[\"pageLoadStrategy\"] = \"none\""
      ],
      "metadata": {
        "id": "DuNhU4_m7r_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 넥스트 유니콘 로그인 및 파인더 페이지 진입\n",
        "try:\n",
        "    crawler = webdriver.Chrome(options=chrome_options)\n",
        "except:\n",
        "    chromedriver_autoinstaller.install(True)\n",
        "    crawler = webdriver.Chrome(options=chrome_options)\n",
        "crawler.get('https://www.nextunicorn.kr/login')\n",
        "time.sleep(1)\n",
        "\n",
        "# 로그인\n",
        "inputid = crawler.find_element(By.CSS_SELECTOR, \"#__next > div.css-hykhfv.e1qaxoiy0 > div > div > div.css-p0yqi0.ezpvhaw0 > div > div > div > form > div:nth-child(1) > div > div > div > input\")\n",
        "inputid.send_keys(\"___아이디 입력란___\")\n",
        "\n",
        "inputpw = crawler.find_element(By.CSS_SELECTOR, \"#__next > div.css-hykhfv.e1qaxoiy0 > div > div > div.css-p0yqi0.ezpvhaw0 > div > div > div > form > div:nth-child(3) > div > div > div > input\")\n",
        "inputpw.send_keys(\"___PW 입력란___\")\n",
        "\n",
        "time.sleep(1)\n",
        "loginBtn = crawler.find_element(By.CSS_SELECTOR, \"#__next > div.css-hykhfv.e1qaxoiy0 > div > div > div.css-p0yqi0.ezpvhaw0 > div > div > div > form > button\")\n",
        "loginBtn.click()\n",
        "\n",
        "# 팝업창 닫기\n",
        "time.sleep(3)\n",
        "crawler.find_element(By.CLASS_NAME, 'css-v6ukax').click()\n",
        "\n",
        "time.sleep(1)\n",
        "finderBtn = crawler.find_element(By.CSS_SELECTOR, '#menu-finder').click()"
      ],
      "metadata": {
        "id": "EpYKgamb7tAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기업 페이지 url 수집"
      ],
      "metadata": {
        "id": "cW4dcomK8clY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_index = list(range(1, 1201))\n",
        "url_list = []\n",
        "\n",
        "try:\n",
        "    for i in tq(total_index):\n",
        "\n",
        "        # 더보기 버튼 클릭\n",
        "        if i != 0 and i % 12 == 0:\n",
        "            view_mort_btn = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-w0y1nt-2.hyyQzY > div > div > div.sc-wmx587-0.kZYyTF > div > div:nth-child(3) > div.sc-wmx587-2.gipIFo > button')\n",
        "            view_mort_btn.click()\n",
        "            time.sleep(2)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # 기업명, URL 수집\n",
        "        try:\n",
        "            link = crawler.find_element(By.CSS_SELECTOR, f'#modal-wrapper > div > div.sc-w0y1nt-2.hyyQzY > div > div > div.sc-wmx587-0.kZYyTF > div > div:nth-child(3) > div.sc-wmx587-2.gipIFo > div.sc-1xk12lb-0.dbdPec > a:nth-child({i})').get_attribute('href')\n",
        "            title = crawler.find_element(By.XPATH, f'//*[@id=\"modal-wrapper\"]/div/div[2]/div/div/div[3]/div/div[3]/div[2]/div[3]/a[{i}]/div/div[2]/div[1]/div[1]/span').text\n",
        "        except:\n",
        "            link = 0\n",
        "            title = 0\n",
        "\n",
        "        url_list.append([title, link])\n",
        "\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# URL 저장\n",
        "data_set = pd.DataFrame(url_list, columns = ['회사명', 'url'])\n",
        "data_set.to_csv(\"___파일 경로___\", index=False)"
      ],
      "metadata": {
        "id": "Ac8m9sms8TYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기업 상세페이지 내 데이터 수집\n",
        "- 회사명 title\n",
        "- 한줄소개 desc\n",
        "- 회사정보 company_info\n",
        "- 설립일 company_date\n",
        "- 홈페이지 company_homepage\n",
        "- 카테고리 company_category\n",
        "- 기술 company_tech\n",
        "- 누적투자금 company_funding\n",
        "- 최근투자일 company_funding_date\n",
        "- 투자라운드 company_stage\n",
        "- 제품소개 company_pruduct\n",
        "- url company_current_url"
      ],
      "metadata": {
        "id": "HbMJI9Ks962X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_list = pd.read_csv(\"___파일 경로___\")\n",
        "\n",
        "crawler.implicitly_wait(0.5)\n",
        "result = []\n",
        "len_url = range(len(url_list))\n",
        "\n",
        "try:\n",
        "    for i in tq(len_url):\n",
        "\n",
        "        # url 접속\n",
        "        crawler.get(url_list['url'][i])\n",
        "        time.sleep(1)\n",
        "\n",
        "        # 기본정보 수집\n",
        "        title = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1au6rf5-0.dfyEHV > div.sc-1tgp7us-0.kkEMVy > div > div > div.sc-1jo1n25-0.eKTDmM > h1').text\n",
        "        try:\n",
        "            desc = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1au6rf5-0.dfyEHV > div.sc-1tgp7us-0.kkEMVy > div > div > div.sc-1jo1n25-0.eKTDmM > div > p').text\n",
        "        except:\n",
        "            desc = 0\n",
        "        try:\n",
        "            company_info = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1d5h8cg-0.keYmOz > div > section.sc-tzw65z-0 > div').text\n",
        "        except:\n",
        "            company_info = 0\n",
        "        try:\n",
        "            company_date = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1f7t804-0.bMzzFA > div.sc-sku89p-0.imGnwh > div.sc-sku89p-2.gapIhE > div:nth-child(2) > div:nth-child(2) > div.sc-15mb17d-2.kPZzPy').text\n",
        "        except:\n",
        "            company_date = 0\n",
        "        try:\n",
        "            company_homepage = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1f7t804-0.bMzzFA > div.sc-sku89p-0.imGnwh > div.sc-sku89p-2.gapIhE > div:nth-child(2) > div:nth-child(3) > a').get_attribute(\"href\")\n",
        "        except:\n",
        "            company_homepage = 0\n",
        "        try:\n",
        "            company_category = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1f7t804-0.bMzzFA > div.sc-sku89p-0.imGnwh > div.sc-sku89p-2.gapIhE > div.sc-1sr3kif-0.efXkfK').text\n",
        "        except:\n",
        "            company_category = 0\n",
        "        try:\n",
        "            company_tech = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1f7t804-0.bMzzFA > div.sc-sku89p-0.imGnwh > div.sc-sku89p-2.gapIhE > div.sc-1lxj15l-0.hgaLMa').text\n",
        "        except:\n",
        "            company_tech = 0\n",
        "\n",
        "        # 기업 페이지 > 투자 페이지 진입\n",
        "        investBtn = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-zm58l9-0.kIQHCV > div > button:nth-child(2)')\n",
        "        investBtn.click()\n",
        "\n",
        "        # 기업 투자 정보 수집\n",
        "        try:\n",
        "            company_funding = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1d5h8cg-0.keYmOz > section > div.sc-fxsdks-0.jcsboE > div:nth-child(1) > div.sc-fxsdks-2.iiScRC').text\n",
        "        except:\n",
        "            company_funding = 0\n",
        "        try:\n",
        "            company_funding_date = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1d5h8cg-0.keYmOz > section > div.sc-1ew9kdj-0.ejNJNv > div > div.sc-1g6qmza-3.iPSlxv > div.sc-1g6qmza-6.fjsLvm').text\n",
        "        except:\n",
        "            company_funding_date = 0\n",
        "        try:\n",
        "            company_stage = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1d5h8cg-0.keYmOz > section > div.sc-fxsdks-0.jcsboE > div:nth-child(2) > div.sc-fxsdks-2.iiScRC').text\n",
        "        except:\n",
        "            company_stage = 0\n",
        "\n",
        "        # 기업 페이지 > 제품 페이지 진입\n",
        "        productBtn = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-zm58l9-0.kIQHCV > div > button:nth-child(3)')\n",
        "        productBtn.click()\n",
        "\n",
        "        # 기업 제품 정보 수집\n",
        "        try:\n",
        "            company_pruduct = crawler.find_element(By.CSS_SELECTOR, '#modal-wrapper > div > div.sc-1tgp7us-0.sc-c899et-1.kkEMVy.cSpElH > div.sc-1d5h8cg-0.keYmOz > section > section > div > div > button').text\n",
        "        except:\n",
        "            company_pruduct = 0\n",
        "\n",
        "        company_current_url = url_list['url'][i]\n",
        "\n",
        "        # 데이터 저장\n",
        "        result.append([title, desc, company_info, company_date, company_homepage, company_category, company_tech, company_funding, company_funding_date, company_stage, company_pruduct, company_current_url])\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# csv 저장\n",
        "data_set = pd.DataFrame(result, columns = ['회사명', '한줄소개', '회사정보', '설립일', '홈페이지', '카테고리', '기술', '누적투자금', '최근투자일', '투자라운드', '제품소개', 'url'])\n",
        "data_set.to_csv(\"/Users/yug/Downloads/nextunicorn_data_full.csv\", index=False)"
      ],
      "metadata": {
        "id": "8g0QXR3z8B60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-2. L 플랫폼: 신규 어플 중심 DB (200여개 데이터 수집)"
      ],
      "metadata": {
        "id": "fG9V4eviPWcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install selenium\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from selenium import webdriver\n",
        "\n",
        "url = 'https://letspl.me/product'\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(url)\n",
        "\n",
        "# scroll down to the end of the mainpage\n",
        "max_try = 30\n",
        "for _ in range(max_try):\n",
        "    body = driver.find_element(By.CSS_SELECTOR, \"body\")\n",
        "    body.send_keys(Keys.PAGE_DOWN)\n",
        "    body.send_keys(Keys.PAGE_DOWN)\n",
        "    body.send_keys(Keys.PAGE_DOWN)\n",
        "    time.sleep(1)\n",
        "\n",
        "# screen shot to check if it's the bottom of the site\n",
        "driver.save_screenshot('webpage.png')\n",
        "\n",
        "# click to access each product's data (product name, category, information)\n",
        "for i:\n",
        "    driver.find_element(By.XPATH, '//*[@id=\"__next\"]/section/div[2]/div[6]/div[2]/f\"div{i}/div[3]/div[1]/div[2]/button').click()\n",
        "    product = driver.find_elements(By.CLASS_NAME, 'expoTitleWrap')\n",
        "    category = driver.find_elements(By.CLASS_NAME, 'expoBooth')\n",
        "    info = driver.find_elements(By.CLASS_NAME, 'expoServiceInfoBooth')\n",
        "    driver.back()\n",
        "## access failed due to the UI of the site -> data was extracted manually"
      ],
      "metadata": {
        "id": "4efRDMPwPdvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-3. D 플랫폼: 신규 어플 중심 DB (200여개 데이터 수집)"
      ],
      "metadata": {
        "id": "7NMC4dwITT38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0\"\n",
        "options.add_argument('user-agent=' + user_agent)\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "url = 'https://disquiet.io/?type=product'\n",
        "driver = Chrome()\n",
        "driver.get(url)\n",
        "\n",
        "texts = []\n",
        "\n",
        "data = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div/div[2]/div[2]/div[1]/div/div[3]').text\n",
        "driver.back()\n",
        "texts.append(data)\n",
        "print(data)\n",
        "\n",
        "texts.apply(lambda x : x.split('프로덕트에 대한 자세한 설명' and '사용해 보러 가기'))"
      ],
      "metadata": {
        "id": "Jy4APFbZTZMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터 전처리"
      ],
      "metadata": {
        "id": "nHso-DwW_fbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "#파일 불러오기\n",
        "product_df = pd.read_csv('merged_file.csv', encoding='utf-8')\n",
        "product_df\n",
        "\n",
        "#결측치 확인 및 제거\n",
        "product_df.isnull().sum()\n",
        "\n",
        "product_df.dropna(inplace=True)\n",
        "\n",
        "#Info 토큰화\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "# Okt 객체 생성\n",
        "okt = Okt()\n",
        "\n",
        "# Okt를 사용한 토큰화 함수\n",
        "def okt_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = okt.morphs(text, stem=True)\n",
        "    return tokens\n",
        "\n",
        "# 텍스트에서 줄바꿈 문자를 제거하는 함수\n",
        "def remove_newlines(text):\n",
        "    # 줄바꿈 문자를 공백으로 대체\n",
        "    cleaned_text = re.sub(r'\\n+', ' ', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# 텍스트 처리 전에 줄바꿈 문자 제거 적용\n",
        "product_df['info'] = product_df['info'].apply(remove_newlines)\n",
        "\n",
        "# Okt 토큰화 적용\n",
        "product_df['tokenized_info'] = product_df['info'].apply(okt_tokenize)\n",
        "\n",
        "product_df['tokenized_info'].head()\n",
        "\n",
        "# 불용어 제거\n",
        "with open('한국어불용어.txt', 'r', encoding='utf-8') as file:\n",
        "    stopwords = [line.strip() for line in file]\n",
        "\n",
        "def remove_stopwords_and_single_chars(tokens):\n",
        "    return [token for token in tokens if token not in stopwords and len(token) > 1]\n",
        "\n",
        "product_df['cleaned_info'] = product_df['tokenized_info'].apply(remove_stopwords_and_single_chars)\n",
        "product_df['cleaned_info'].head(20)\n",
        "\n",
        "#카테고리 불용어 제거\n",
        "stopwords = [\"기타\"]\n",
        "\n",
        "# Function to clean and split the category column\n",
        "def clean_and_split(category):\n",
        "    # Use regular expression to remove special characters and split by spaces\n",
        "    parts = re.findall('[가-힣a-zA-Z0-9&]+', category)\n",
        "\n",
        "    # 불용어 제거\n",
        "    parts = [word for word in parts if word not in stopwords]\n",
        "\n",
        "    return ', '.join(set(parts))\n",
        "\n",
        "# Apply the function to each row in the 'category' column and create 'cleaned_cat' column\n",
        "product_df['cleaned_cat'] = product_df['category'].apply(clean_and_split)\n",
        "product_df['cleaned_cat'].tail(20)\n",
        "\n",
        "\n",
        "## ======= detele this part ======= ##\n",
        "## remain all tokens/words excluding some stopwords such as \"기타\", \"제조\"\n",
        "#카테고리 뒤에서 3개만 저장코드\n",
        "def extract_last_two_categories(category):\n",
        "    parts = category.split(', ')  # 쉼표와 공백으로 분할\n",
        "    if len(parts) >= 2:\n",
        "        return ', '.join(parts[-3:])  # 마지막 두 부분만 추출하여 결합\n",
        "    else:\n",
        "        return category  # 두 개 미만의 요소가 있는 경우 원래의 카테고리 반환\n",
        "product_df['cleaned_c'] = product_df['cleaned_cat'].apply(extract_last_two_categories)\n",
        "product_df['cleaned_c'].head(20)\n",
        "## ======= detele this part ======= ##\n",
        "\n",
        "\n",
        "#파일 저장\n",
        "product_df.to_csv('merged_file.csv', index=False)\n",
        "\n",
        "#필요없는 컬럼 삭제\n",
        "# 원본 데이터 파일 불러오기\n",
        "file_path = 'merged_file.csv'\n",
        "product_df = pd.read_csv(file_path)\n",
        "\n",
        "# 필요한 데이터프레임 조작 수행\n",
        "product_df.drop(['cleaned_c'], axis=1, inplace=True)\n",
        "\n",
        "# 수정된 데이터프레임을 원본 파일에 다시 저장 (덮어쓰기)\n",
        "product_df.to_csv(file_path, index=False)\n",
        "\n",
        "#받은 데이터 정리 및 합치기\n",
        "df = pd.read_csv('nextunicorn_data_240109_1.csv', encoding='utf-8')\n",
        "\n",
        "# 필요한 컬럼들을 하나의 컬럼으로 합치기\n",
        "df['info'] = df['한줄소개'] + \" \" + df['회사정보'] + \" \" + df['제품소개']\n",
        "\n",
        "# 카테고리와 기술을 합쳐 하나의 컬럼으로 만들기\n",
        "df['category'] = df['카테고리'] + \" \" + df['기술']\n",
        "\n",
        "df['product'] = df['회사명']\n",
        "\n",
        "# 결과 확인\n",
        "df[['info', 'category', 'product']].head()\n",
        "\n",
        "selected_columns = ['info', 'category', 'product']\n",
        "df[selected_columns].to_csv('file3.csv', encoding='utf-8', index=False)\n",
        "\n",
        "# 파일 합치기\n",
        "df1 = pd.read_csv('file.csv', encoding='utf-8')\n",
        "\n",
        "df2 = pd.read_csv('file2.csv', encoding='utf-8')\n",
        "\n",
        "df3 = pd.read_csv('file3.csv', encoding='utf-8')\n",
        "\n",
        "merged_df = pd.concat([df1, df2, df3])\n",
        "\n",
        "# 합쳐진 파일 저장\n",
        "merged_df.to_csv('merged_file.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "DAs9O-Xi_uME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 아이디어 생성 모델"
      ],
      "metadata": {
        "id": "GXroc5GaEFLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "!pip install summa\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "_jhQB4CBERbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf36167-bfa8-4f70-ae58-f87982eb0a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from summa) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy>=0.19->summa) (1.23.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54388 sha256=cb68338d81031daf1db6ea6dec12659ad2973f8d79246cf9dbc8c84a0dee99ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/ca/c5/4958614cfba88ed6ceb7cb5a849f9f89f9ac49971616bc919f\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from konlpy.tag import Okt\n",
        "from summa import keywords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Bidirectional, Dropout, GRU\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText as FT\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "Qb0jpXPeET1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "data = pd.read_csv('merged_file.csv', encoding='utf-8')\n",
        "text = data['cleaned_info'].apply(lambda a : re.findall('[가-힣a-z]+', a))"
      ],
      "metadata": {
        "id": "NGa_ZXwFEXXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Twwk4YRZ4Oen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.apply(lambda a : a.append('<eos>'))"
      ],
      "metadata": {
        "id": "TPyHFHRJEbnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(text)"
      ],
      "metadata": {
        "id": "hv1GYvQLEcdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_n = len(tokenizer.word_index) + 1\n",
        "vocab_n"
      ],
      "metadata": {
        "id": "Ry95LUhkEm33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeded = tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "ISY7ovxYEvSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_len = 10\n",
        "def Padding (text) :\n",
        "    lists = []\n",
        "    for ind in range(1, len(text)) :\n",
        "        if ind < pad_len + 1 :\n",
        "            words = text[:ind + 1]\n",
        "        else :\n",
        "            words = text[ind - pad_len : ind + 1]\n",
        "        lists.append(words)\n",
        "    return pad_sequences(lists, pad_len + 1, padding = 'pre')"
      ],
      "metadata": {
        "id": "mnyBWA5LEwmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_data = list(map(Padding, embeded))"
      ],
      "metadata": {
        "id": "dW92-UldExrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in t_data :\n",
        "    for j in i :\n",
        "        data.append(j)"
      ],
      "metadata": {
        "id": "pReETcgNEyqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "for i in data :\n",
        "    y.append(np.array(list(i).pop(-1)))\n",
        "    X.append(i[:-1])"
      ],
      "metadata": {
        "id": "4evXIgndE2Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "ldnzl-d4E235"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tf.keras.Input(shape=(pad_len, ))\n",
        "x = tf.keras.layers.Embedding(vocab_n, 256)(input)\n",
        "x = tf.keras.layers.LSTM(512, return_sequences = False)(x)\n",
        "x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
        "output = tf.keras.layers.Dense(vocab_n, activation = 'softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs = input, outputs = output)"
      ],
      "metadata": {
        "id": "47uK80YaE3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.05\n",
        "# lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     0.01, decay_steps=2, decay_rate=0.5, staircase=True)\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "              metrics = ['acc'])"
      ],
      "metadata": {
        "id": "aJoNBQXkE_ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs = 50, batch_size = 4096)"
      ],
      "metadata": {
        "id": "dautVS1nFJMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, tokenizer, current_word): # 모델, 토크나이저, 키워드(단어 여러개 가능)\n",
        "    current_word = current_word.split(' ')\n",
        "\n",
        "    # n번 반복\n",
        "    for _ in range(30):\n",
        "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])\n",
        "        encoded = pad_sequences(encoded, maxlen=pad_len, padding='pre')\n",
        "\n",
        "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        pred = model.predict(encoded)\n",
        "\n",
        "        top_k = np.argsort(pred[0])[:-4:-1]\n",
        "        ind = np.random.choice([0,0,0,0,1,1,2]) # random으로 상위 ind위의 단어를 result로 할당한다\n",
        "        result = tokenizer.index_word.get(top_k[ind])\n",
        "\n",
        "        if result == 'eos' :\n",
        "            return current_word\n",
        "        words = result.split(' ')\n",
        "\n",
        "        if len(words) != 1 and result == words[-1] :\n",
        "            continue\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word.append(result)\n",
        "\n",
        "    return current_word"
      ],
      "metadata": {
        "id": "Nx4A0vaTFKbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import keywords\n",
        "import gensim\n",
        "from konlpy.tag import Hannanum, Okt, Kkma\n",
        "from gensim.models import Word2Vec\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "GpGqNrEkFwbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Extractor(list) : # keyword는 ['단어', '단어', ...] 형식으로 받는다\n",
        "    words_n = 3 # 뽑을 키워드 갯수\n",
        "    keyword = []\n",
        "\n",
        "    string = okt.nouns(' '.join(list)) # 명사만 찾는다\n",
        "    string = ' '.join(string) # findall을 하기위해 묶는다\n",
        "    keyword = ' '.join(re.findall('\\w{2,}', string.lower())) # 1글자인 단어들 삭제\n",
        "\n",
        "    if len(set(keyword)) < words_n :\n",
        "        words_n = len(set(keyword)) # 단어 갯수가 적을 경우 뽑을 키워드 갯수를 낮춘다\n",
        "\n",
        "    try :\n",
        "        key = keywords.keywords(keyword, words = words_n) # 키워드 뽑기\n",
        "    except :\n",
        "        words_n -= 1\n",
        "        key = keywords.keywords(keyword, words = words_n)\n",
        "\n",
        "    key = key.replace('\\n', ' ')\n",
        "    return key.split(' ')[:words_n]"
      ],
      "metadata": {
        "id": "z852qWQUFxor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data['cleaned_info'].apply(lambda a : re.findall('[가-힣a-z]+', a)).to_list()"
      ],
      "metadata": {
        "id": "rVER42YvF248"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = train_data + sent.apply(lambda a : re.findall('[가-힣a-z]+', a)).to_list()"
      ],
      "metadata": {
        "id": "Svy-JhyByyhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences = train_data, vector_size = 50, window = 5, min_count = 0, workers = 4, sg = 0)"
      ],
      "metadata": {
        "id": "SGQNiQyuF5dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Simmilarity(keyword1, keyword2) : # 문장1의 키워드, 문장2의 키워드를 받는다. keyword2에 사용자 키워드를 가져와야 한다\n",
        "    vector_dic = {}\n",
        "    score = 0\n",
        "    n = len(keyword1)\n",
        "    for i in [keyword1, keyword2] :\n",
        "        for ind, keyword in enumerate(i) :\n",
        "            if keyword in w2v_model.wv.key_to_index.keys() :\n",
        "                i[ind] = np.round(w2v_model.wv[keyword], 3) # 벡터값을 vector_dic에 저장\n",
        "            else :\n",
        "                n -= 1\n",
        "\n",
        "    for i in keyword1 :\n",
        "        for j in keyword2 :\n",
        "            try :\n",
        "                score += i.dot(j.T) # scaled dot\n",
        "            except :\n",
        "                continue\n",
        "    if n == 0:\n",
        "        return 0\n",
        "    return np.round(score / n, 5) # attention score를 다 더하고 query 갯수만큼 나누었다"
      ],
      "metadata": {
        "id": "zuiwdYkAF76o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Simmilarity(['일기', '기록', '내용'], ['롤플레잉', '캐릭터', '채팅']) # 사용 예시"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KVvKZe-QOTg",
        "outputId": "b7ed4b7b-1b3f-4721-8b3b-950a0ab7cc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27.64695"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실행파일"
      ],
      "metadata": {
        "id": "xP7uZgYna53X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pre.txt', 'r') as f:\n",
        "    dic = []\n",
        "    for i in f :\n",
        "        dic.append(i.replace('\\n', ''))\n",
        "\n",
        "dic = pd.DataFrame(dic, columns = ['0'])"
      ],
      "metadata": {
        "id": "QmZtyPlav2ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dic = pd.read_csv('gen_data.csv') # 코딩을 위한 임시 데이터\n",
        "sent = dic['0']\n",
        "\n",
        "key_dic = {}\n",
        "for ind, i in enumerate(sent) :\n",
        "    i = i.split()\n",
        "    key_dic[ind] = Extractor(i)"
      ],
      "metadata": {
        "id": "y1wkziT4_dDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fav = input('좋아하는 것을 입력해주세요 :')\n",
        "tel = input('잘하는 것을 입력해주세요 :')\n",
        "\n",
        "fav = Extractor(fav) # 잘하는 것, 좋아하는 것 키워드 추출\n",
        "tel = Extractor(tel)\n",
        "\n",
        "ideal_vec = fav + tel\n",
        " # 키워드를 단순하게 합침으로써, 겹치는 키워드가 있다면 유사도 계산 때 더 많은 영향을 줄 것이다\n",
        "sim_dic = {}\n",
        "for i in key_dic.keys() :\n",
        "    if key_dic[i] == ['']:\n",
        "        sim_dic[i] = 0\n",
        "        continue\n",
        "    ideal_vec = fav + tel\n",
        "    sim_dic[i] = Simmilarity(key_dic[i], ideal_vec)\n",
        "\n",
        "sorted_dict = sorted(sim_dic.items(), key=lambda item: item[1])\n",
        "\n",
        "for i, j in sorted_dict[-1:-6:-1] :\n",
        "    print(dic.iloc[i]['0'])"
      ],
      "metadata": {
        "id": "fXBSCgqva9PU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}